{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrogram Channels U-net\n",
    "\n",
    "* Spectrogram-Channels U-Net: A Source Separation Model Viewing Each Channel as the Spectrogram of Each Source, [arXiv:1810.11520](https://arxiv.org/abs/1810.11520)\n",
    "  * Jaehoon Oh∗, Duyeon Kim∗, Se-Young Yun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import PIL\n",
    "import imageio\n",
    "from IPython import display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Flags (hyperparameter configuration)\n",
    "model_name = 'spectrogram_unet'\n",
    "train_dir = 'train/' + model_name + '/exp1/'\n",
    "max_epochs = 200\n",
    "save_model_epochs = 20\n",
    "print_steps = 1\n",
    "batch_size = 8\n",
    "learning_rate = 2e-4\n",
    "N = 100 # number of samples in train_dataset\n",
    "\n",
    "BUFFER_SIZE = N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset with `tf.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './datasets/spectrogram/'\n",
    "train_data_filenames = [os.path.join(data_path, 'train', name)\n",
    "                        for name in os.listdir(os.path.join(data_path, 'train')) if 'tfrecord' in name]\n",
    "for name in train_data_filenames:\n",
    "  print(name)\n",
    "\n",
    "#test_data_filenames = [os.path.join(data_path, 'train', name)\n",
    "#                       for name in os.listdir(os.path.join(data_path, 'test')) if 'tfrecord' in name]\n",
    "#for name in   print(nametest_data_filenames:\n",
    "#  print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms a scalar string `example_proto` into a pair of a scalar string and\n",
    "# a scalar integer, representing an spectragram and corresponding informations, respectively.\n",
    "def _parse_function(example_proto):\n",
    "  features = {'spec_raw': tf.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "              'frequency_bin': tf.FixedLenFeature([], tf.int64, default_value=0),\n",
    "              'time_step': tf.FixedLenFeature([], tf.int64, default_value=0),\n",
    "              'channel': tf.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "              'track_number': tf.FixedLenFeature([], tf.int64, default_value=0),\n",
    "              'split_number': tf.FixedLenFeature([], tf.int64, default_value=0),}\n",
    "  \n",
    "  parsed_features = tf.parse_single_example(example_proto, features)\n",
    "\n",
    "  spec_raw = tf.decode_raw(parsed_features[\"spec_raw\"], out_type=tf.float32)\n",
    "  frequency_bin = tf.cast(parsed_features[\"frequency_bin\"], dtype=tf.int32)\n",
    "  time_step = tf.cast(parsed_features[\"time_step\"], dtype=tf.int32)\n",
    "  #channel = tf.cast(parsed_features[\"channel\"], dtype=tf.string)\n",
    "  #track_number = tf.cast(parsed_features[\"track_number\"], dtype=tf.int32)\n",
    "  #split_number = tf.cast(parsed_features[\"split_number\"], dtype=tf.int32)\n",
    "  \n",
    "  num_channels = 6 # for [mixtures, vocals, drums, basses, others, accompaniments]\n",
    "  spec_raw = tf.reshape(spec_raw, shape=[frequency_bin, time_step, num_channels])\n",
    "\n",
    "  return spec_raw, time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _augmentation_function(spec_raw, time_step):\n",
    "  \"\"\"Random cropping for data augmentation\n",
    "  \"\"\"\n",
    "  target_time_step = 128 # our input size\n",
    "  available_time_step = time_step - target_time_step\n",
    "  \n",
    "  crop_index = tf.random_uniform(shape=[]) * tf.cast(available_time_step, dtype=tf.float32)\n",
    "  crop_index = tf.cast(crop_index, dtype=tf.int32)\n",
    "  spec_raw_crop = spec_raw[:, crop_index:crop_index+target_time_step, :]\n",
    "  \n",
    "  return spec_raw_crop[..., 0:1], spec_raw_crop[..., 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(train_data_filenames)\n",
    "train_dataset = train_dataset.map(_parse_function)\n",
    "train_dataset = train_dataset.map(_augmentation_function)\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(tf.keras.Model):\n",
    "  def __init__(self, num_filters, size):\n",
    "    super(ConvBlock, self).__init__()\n",
    "    self.conv1 = layers.Conv2D(filters=num_filters,\n",
    "                               kernel_size=(size, size),\n",
    "                               padding='same',\n",
    "                               use_bias=False)\n",
    "    self.batchnorm1 = layers.BatchNormalization()\n",
    "    self.conv2 = layers.Conv2D(filters=num_filters,\n",
    "                               kernel_size=(size, size),\n",
    "                               padding='same',\n",
    "                               use_bias=False)\n",
    "    self.batchnorm2 = layers.BatchNormalization()\n",
    "  \n",
    "  def call(self, x, training=True):\n",
    "    x = self.conv1(x)\n",
    "    x = self.batchnorm1(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.batchnorm2(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(tf.keras.Model):\n",
    "  def __init__(self, num_filters, size):\n",
    "    super(EncoderBlock, self).__init__()\n",
    "    self.conv_block = ConvBlock(num_filters, 3)\n",
    "    \n",
    "  def call(self, x, training=True):\n",
    "    encoder = self.conv_block(x, training=training)\n",
    "    encoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
    "  \n",
    "    return encoder_pool, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTransposeBlock(tf.keras.Model):\n",
    "  def __init__(self, num_filters, size):\n",
    "    super(ConvTransposeBlock, self).__init__()\n",
    "    self.convT1 = layers.Conv2DTranspose(filters=num_filters,\n",
    "                                         kernel_size=(size, size),\n",
    "                                         padding='same',\n",
    "                                         use_bias=False)\n",
    "    self.batchnorm1 = layers.BatchNormalization()\n",
    "    self.convT2 = layers.Conv2DTranspose(filters=num_filters,\n",
    "                                         kernel_size=(size, size),\n",
    "                                         padding='same',\n",
    "                                         use_bias=False)\n",
    "    self.batchnorm2 = layers.BatchNormalization()\n",
    "  \n",
    "  def call(self, x, training=True):\n",
    "    x = self.convT1(x)\n",
    "    x = self.batchnorm1(x, training=training)\n",
    "    x = tf.nn.relu(x)    \n",
    "    x = self.convT2(x)\n",
    "    x = self.batchnorm2(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(tf.keras.Model):\n",
    "  def __init__(self, num_filters, size):\n",
    "    super(DecoderBlock, self).__init__()\n",
    "    self.convT = layers.Conv2DTranspose(filters=num_filters,\n",
    "                                        kernel_size=(size+2, size+2),\n",
    "                                        strides=(2, 2),\n",
    "                                        padding='same',\n",
    "                                        use_bias=False)\n",
    "    self.batchnorm = layers.BatchNormalization()\n",
    "    self.dropout = layers.Dropout(0.4)\n",
    "    self.convT_block = ConvTransposeBlock(num_filters, size)\n",
    "  \n",
    "  def call(self, input_tensor, concat_tensor, training=True):\n",
    "    # Upsampling\n",
    "    x = self.convT(input_tensor)\n",
    "    x = self.batchnorm(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    # concatenate\n",
    "    x = tf.concat([x, concat_tensor], axis=-1)\n",
    "    \n",
    "    # just two consecutive conv_transpose\n",
    "    x = self.convT_block(x, training=training)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramChannelsUNet(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(SpectrogramChannelsUNet, self).__init__()\n",
    "    self.down1 = EncoderBlock(32, 3)\n",
    "    self.down2 = EncoderBlock(64, 3)\n",
    "    self.down3 = EncoderBlock(128, 3)\n",
    "    self.down4 = EncoderBlock(256, 3)\n",
    "    self.center = ConvBlock(512, 3)\n",
    "\n",
    "    self.up1 = DecoderBlock(256, 3)\n",
    "    self.up2 = DecoderBlock(128, 3)\n",
    "    self.up3 = DecoderBlock(64, 3)\n",
    "    self.up4 = DecoderBlock(32, 3)\n",
    "\n",
    "    self.last = layers.Conv2D(filters=5,\n",
    "                              kernel_size=(1, 1),\n",
    "                              padding='same')\n",
    "  \n",
    "  @tf.contrib.eager.defun\n",
    "  def call(self, x, training):\n",
    "    # x shape == (bs, 1024, 256, 1)\n",
    "    x1_pool, x1 = self.down1(x, training=training) # (bs, 512, 128, 32)\n",
    "    x2_pool, x2 = self.down2(x1_pool, training=training) # (bs, 256, 64, 64)\n",
    "    x3_pool, x3 = self.down3(x2_pool, training=training) # (bs, 128, 32, 128)\n",
    "    x4_pool, x4 = self.down4(x3_pool, training=training) # (bs, 64, 16, 256)\n",
    "    x_center = self.center(x4_pool, training=training) # (bs, 64, 16, 512)\n",
    "\n",
    "    x5 = self.up1(x_center, x4, training=training) # (bs, 128, 32, 256)\n",
    "    x6 = self.up2(x5, x3, training=training) # (bs, 256, 64, 128)\n",
    "    x7 = self.up3(x6, x2, training=training) # (bs, 512, 128, 64)\n",
    "    x8 = self.up4(x7, x1, training=training) # (bs, 1024, 256, 32)\n",
    "\n",
    "    x_last = self.last(x8) # (bs, 1024, 256, 5)\n",
    "    x_last = tf.math.sigmoid(x_last)\n",
    "\n",
    "    return x_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpectrogramChannelsUNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in train_dataset.take(1):\n",
    "  x = model(inputs, training=True)\n",
    "  y = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining loss functions and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(y_true, y_pred):\n",
    "  loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "  loss = tf.reduce_mean(loss)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = train_dir\n",
    "if not tf.gfile.Exists(checkpoint_dir):\n",
    "  tf.gfile.MakeDirs(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.info('Start Training.')\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "for epoch in range(max_epochs):\n",
    "  \n",
    "  for mixtures, targets in train_dataset:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = model(mixtures, training=True)\n",
    "      loss = bce_loss(targets, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.variables), global_step=global_step)\n",
    "        \n",
    "    epochs = global_step.numpy() * batch_size / float(N)\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    if global_step.numpy() % print_steps == 0:\n",
    "      display.clear_output(wait=True)\n",
    "      examples_per_sec = batch_size / float(duration)\n",
    "      print(\"Epochs: {:.2f} global_step: {} loss: {:.3f} ({:.2f} examples/sec; {:.3f} sec/batch)\".format(\n",
    "                epochs, global_step.numpy(), loss, examples_per_sec, duration))\n",
    "      # generate sample image from random test image\n",
    "      # the training=True is intentional here since\n",
    "      # we want the batch statistics while running the model\n",
    "      # on the test dataset. If we use training=False, we will get \n",
    "      # the accumulated statistics learned from the training dataset\n",
    "      # (which we don't want)\n",
    "      #for test_input, test_target in test_dataset.take(1):\n",
    "      #  prediction = generator(test_input, training=True)\n",
    "      #  print_or_save_sample_images(test_input, test_target, prediction)\n",
    "\n",
    "#   if (epoch + 1) % save_images_epochs == 0:\n",
    "#     display.clear_output(wait=True)\n",
    "#     print(\"This images are saved at {} epoch\".format(epoch+1))\n",
    "#     prediction = generator(constant_test_input, training=True)\n",
    "#     print_or_save_sample_images(constant_test_input, constant_test_target, prediction,\n",
    "#                                 is_save=True, epoch=epoch+1, checkpoint_dir=checkpoint_dir)\n",
    "\n",
    "#   # saving (checkpoint) the model every save_epochs\n",
    "#   if (epoch + 1) % save_model_epochs == 0:\n",
    "#     checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the entire test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
