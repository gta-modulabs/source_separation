{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert wave to tfrecords format\n",
    "\n",
    "* Sampling down: 44100Hz\n",
    "* musdb data\n",
    "  * `train_data`: 100 tracks\n",
    "    * the shortest time track: 12.91 sec for 66th track\n",
    "    * the largest time track: 628.38 sec for 51th track\n",
    "  * `test_data`: 50 tracks\n",
    "    * the shortest time track: 76.00 sec for 29th track\n",
    "    * the largest time track: 430.20 sec for 15th track\n",
    "* So, I will split wave files into `5 second` segments\n",
    "  * for example, track = 17 sec\n",
    "  * part1: [0:5] sec, part2: [5:10] sec, part3: [10:17] sec\n",
    "  * add ramainder part (last 2sec segmentation) to last part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import musdb\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = musdb.DB(root_dir='./datasets/musdb18/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training tracks\n",
    "split_name = 'train'\n",
    "assert split_name in ['train', 'test']\n",
    "\n",
    "tracks = mus.load_mus_tracks(subsets=[split_name])\n",
    "print(type(tracks))\n",
    "print(len(tracks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for total time (minute, sec) info\n",
    "# min_sec = 1000.0\n",
    "# max_sec = 0.0\n",
    "# for i, track in enumerate(tracks):\n",
    "#   sec = track.audio.T.shape[1] / 44100\n",
    "#   if sec < min_sec:\n",
    "#     min_sec = sec\n",
    "#   if sec > max_sec:\n",
    "#     max_sec = sec\n",
    "  \n",
    "#   minute = int(sec / 60)\n",
    "#   sec = sec - minute * 60\n",
    "#   print(\"{}th: {} min {:.2f} sec\".format(i, minute, sec))\n",
    "# print(\"the shortest time track: {:.2f} sec\".format(min_sec))\n",
    "# print(\"the largest time track: {:.2f} sec\".format(max_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks[0].targets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "print(tracks[index].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listen the track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original track - mixture\n",
    "display.Audio(tracks[index].audio.T, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tracks[index].audio.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to listen in each stem source then uncomment them\n",
    "# display.Audio(tracks[index].targets['vocals'].audio.T, rate=44100)\n",
    "# display.Audio(tracks[index].targets['drums'].audio.T, rate=44100)\n",
    "# display.Audio(tracks[index].targets['bass'].audio.T, rate=44100)\n",
    "# display.Audio(tracks[index].targets['other'].audio.T, rate=44100)\n",
    "# display.Audio(tracks[index].targets['accompaniment'].audio.T, rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot for short time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate to left and right channels\n",
    "second = 10\n",
    "left_wave = tracks[index].audio.T[0][:44100 * second]\n",
    "right_wave = tracks[index].audio.T[1][:44100 * second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the each channel\n",
    "plt.figure(figsize=[18, 3])\n",
    "plt.plot(left_wave)\n",
    "\n",
    "plt.figure(figsize=[18, 3])\n",
    "plt.plot(right_wave)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(left_wave.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short-time Fourier Transform\n",
    "# n_fft: number of samples used to calculate fft\n",
    "# hop_length: like concept of stride\n",
    "left_stft = librosa.core.stft(left_wave, n_fft=2048, hop_length=512)\n",
    "print(left_stft.shape)\n",
    "print(type(left_stft[0, 0]))\n",
    "print(left_stft[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_abs = abs(left_stft)\n",
    "librosa.display.specshow(left_abs)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "librosa.display.specshow(librosa.amplitude_to_db(left_abs, ref=np.max))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram using normalize (for maybe standard method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on Tacotron\n",
    "#min_level_db = -100\n",
    "#ref_level_db = 20\n",
    "# our suggestion\n",
    "min_level_db = -110\n",
    "ref_level_db = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram(y):\n",
    "  D = _stft(y)\n",
    "  S = _amp_to_db(np.abs(D)) - ref_level_db\n",
    "  return _normalize(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stft(y):\n",
    "  #n_fft, hop_length, win_length = 2048, 512, 2048\n",
    "  n_fft, hop_length, win_length = 1024, 512, 1024\n",
    "  return librosa.stft(y=y, n_fft=n_fft, hop_length=hop_length, win_length=win_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _amp_to_db(x):\n",
    "  return 20 * np.log10(np.maximum(1e-5, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(S):\n",
    "  return np.clip((S - min_level_db) / -min_level_db, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_spec = spectrogram(left_wave)\n",
    "print(left_spec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "librosa.display.specshow(left_spec)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to tfrecords format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int64_feature(values):\n",
    "  \"\"\"Returns a TF-Feature of int64s.\n",
    "\n",
    "  Args:\n",
    "    values: A scalar or list of values.\n",
    "\n",
    "  Returns:\n",
    "    A TF-Feature.\n",
    "  \"\"\"\n",
    "  if not isinstance(values, (tuple, list)):\n",
    "    values = [values]\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n",
    "\n",
    "\n",
    "def bytes_feature(values):\n",
    "  \"\"\"Returns a TF-Feature of bytes.\n",
    "\n",
    "  Args:\n",
    "    values: A string.\n",
    "\n",
    "  Returns:\n",
    "    A TF-Feature.\n",
    "  \"\"\"\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))\n",
    "\n",
    "\n",
    "def float_feature(values):\n",
    "  \"\"\"Returns a TF-Feature of floats.\n",
    "\n",
    "  Args:\n",
    "    values: A scalar of list of values.\n",
    "\n",
    "  Returns:\n",
    "    A TF-Feature.\n",
    "  \"\"\"\n",
    "  if not isinstance(values, (tuple, list)):\n",
    "    values = [values]\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_dataset_filename(dataset_dir, split_name, shard_id, num_shards):\n",
    "  output_filename = 'wav_%s_%05d-of-%05d.tfrecord' % (\n",
    "      split_name, shard_id, num_shards)\n",
    "  return os.path.join(dataset_dir, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset(split_name, dataset_dir, N, num_shards):\n",
    "  \"\"\"Converts the wav of given tracks to a TFRecord dataset.\n",
    "\n",
    "  Args:\n",
    "    split_name: The name of the dataset, either 'train' or 'validation'.\n",
    "    dataset_dir: The directory where the converted datasets are stored.\n",
    "    N: number of total examples # train: 100, test: 50\n",
    "    num_shards: number of shards\n",
    "  \"\"\"\n",
    "  assert split_name in ['train', 'test']\n",
    "\n",
    "  # data split\n",
    "  dataset_path = os.path.join(dataset_dir, split_name)\n",
    "  print(dataset_path)\n",
    "  if not tf.gfile.Exists(dataset_path):\n",
    "    tf.gfile.MakeDirs(dataset_path)\n",
    "  \n",
    "  # for data suffling\n",
    "  permutation_track_number = np.random.permutation(N)\n",
    "  \n",
    "  num_per_shard = int(N / float(num_shards))\n",
    "  for shard_id in range(num_shards):\n",
    "    output_filename = _get_dataset_filename(\n",
    "              dataset_path, split_name, shard_id, num_shards)\n",
    "    print('Writing', output_filename)\n",
    "\n",
    "    # step 1\n",
    "    with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:\n",
    "      start_ndx = shard_id * num_per_shard\n",
    "      end_ndx = min((shard_id+1) * num_per_shard, N)\n",
    "\n",
    "      for i in range(start_ndx, end_ndx):\n",
    "        sys.stdout.write('\\r>> Converting wave %d/%d shard %d\\n' % (\n",
    "            i+1, N, shard_id))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        mixtures = tracks[permutation_track_number[i]].audio.T\n",
    "        vocals = tracks[permutation_track_number[i]].targets['vocals'].audio.T\n",
    "        drums = tracks[permutation_track_number[i]].targets['drums'].audio.T\n",
    "        basses = tracks[permutation_track_number[i]].targets['bass'].audio.T\n",
    "        others = tracks[permutation_track_number[i]].targets['other'].audio.T\n",
    "        accompaniments = tracks[permutation_track_number[i]].targets['accompaniment'].audio.T\n",
    "        \n",
    "        num_channels, number_of_samples = mixtures.shape\n",
    "        assert (num_channels, number_of_samples) == vocals.shape\n",
    "        assert (num_channels, number_of_samples) == drums.shape\n",
    "        assert (num_channels, number_of_samples) == basses.shape\n",
    "        assert (num_channels, number_of_samples) == others.shape\n",
    "        assert (num_channels, number_of_samples) == accompaniments.shape\n",
    "\n",
    "        sources = [mixtures, vocals, drums, basses, others, accompaniments]\n",
    "        time_step_for_one_example = 44100 * 5 # 5 sec for one example\n",
    "        num_split = int(number_of_samples / time_step_for_one_example)\n",
    "        print(\"{}th track; num_split: {}\".format(permutation_track_number[i], num_split))\n",
    "\n",
    "        for split_index in range(num_split-1):\n",
    "          if split_index > 2:\n",
    "            break\n",
    "          # step 2\n",
    "          for k, wav in enumerate(sources):\n",
    "            wav_split = wav[:, split_index*time_step_for_one_example:(split_index+1)*time_step_for_one_example]\n",
    "            if k < 1:\n",
    "              wav_concat = np.expand_dims(wav_split, axis=0)\n",
    "            else:\n",
    "              wav_concat = np.concatenate((wav_concat, np.expand_dims(wav_split, axis=0)), axis=0)\n",
    "          \n",
    "          time_step = wav_concat.shape[2]\n",
    "          wav_concat_string = wav_concat.tostring()\n",
    "          print(\"{}th track; {}th split: wav_shape: {}\".format(permutation_track_number[i], split_index, wav_concat.shape))\n",
    "\n",
    "          # step 3:\n",
    "          features = tf.train.Features(feature={'wav_concat': bytes_feature(wav_concat_string),\n",
    "                                                'time_step': int64_feature(time_step),\n",
    "                                                'track_number': int64_feature(permutation_track_number[i]),\n",
    "                                                'split_number': int64_feature(split_index),\n",
    "                                               })\n",
    "\n",
    "          # step 4\n",
    "          example = tf.train.Example(features=features)\n",
    "\n",
    "          # step 5\n",
    "          tfrecord_writer.write(example.SerializeToString())\n",
    "\n",
    "        # merge between last split part and residual part\n",
    "        # step 2\n",
    "        for k, wav in enumerate(sources):\n",
    "          wav_split = wav[:, (num_split-1)*time_step_for_one_example:]\n",
    "          if k < 1:\n",
    "            wav_concat = np.expand_dims(wav_split, axis=0)\n",
    "          else:\n",
    "            wav_concat = np.concatenate((wav_concat, np.expand_dims(wav_split, axis=0)), axis=0)\n",
    "          \n",
    "        time_step = wav_concat.shape[2]\n",
    "        wav_concat_string = wav_concat.tostring()\n",
    "        print(\"{}th track; {}th split: wav_shape: {}\".format(permutation_track_number[i], num_split, wav_concat.shape))\n",
    "\n",
    "        # step 3:\n",
    "        features = tf.train.Features(feature={'wav_concat': bytes_feature(wav_concat_string),\n",
    "                                              'time_step': int64_feature(time_step),\n",
    "                                              'track_number': int64_feature(permutation_track_number[i]),\n",
    "                                              'split_number': int64_feature(num_split),\n",
    "                                             })\n",
    "\n",
    "        # step 4\n",
    "        example = tf.train.Example(features=features)\n",
    "\n",
    "        # step 5\n",
    "        tfrecord_writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_dir = './datasets/tfrecords'\n",
    "NUM_SHARDS = 25 # for train: 25, for test: 10\n",
    "N = 100 # for train: 100, for test: 50\n",
    "convert_dataset(split_name, tfrecords_dir, N, NUM_SHARDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
