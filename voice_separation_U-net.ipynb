{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectrogram Channels U-net\n",
    "\n",
    "* SPECTROGRAM-CHANNELS U-NET: A SOURCE SEPARATION MODEL VIEWING EACH CHANNEL AS THE SPECTROGRAM OF EACH SOURCE, [arXiv:1810.11520](https://arxiv.org/abs/1810.11520)\n",
    "  * Jaehoon Oh∗, Duyeon Kim∗, Se-Young Yun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import PIL\n",
    "#import imageio\n",
    "from IPython import display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Flags (hyperparameter configuration)\n",
    "model_name = 'spectrogram_unet'\n",
    "train_dir = 'train/' + model_name + '/exp1/'\n",
    "max_epochs = 1500\n",
    "save_model_epochs = 20\n",
    "print_steps = 1\n",
    "batch_size = 8\n",
    "learning_rate = 2e-4\n",
    "N = 128 # number of samples in train_dataset\n",
    "\n",
    "BUFFER_SIZE = N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset with `tf.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/tfrecords/train/wav_train_00010-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00003-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00012-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00023-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00017-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00006-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00013-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00016-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00015-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00022-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00001-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00021-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00004-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00020-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00011-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00005-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00019-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00014-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00018-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00007-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00009-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00024-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00002-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00008-of-00025.tfrecord\n",
      "./datasets/tfrecords/train/wav_train_00000-of-00025.tfrecord\n",
      "--------\n",
      "./datasets/tfrecords/test/wav_test_00007-of-00010.tfrecord\n",
      "./datasets/tfrecords/test/wav_test_00004-of-00010.tfrecord\n",
      "./datasets/tfrecords/test/wav_test_00009-of-00010.tfrecord\n",
      "./datasets/tfrecords/test/wav_test_00002-of-00010.tfrecord\n",
      "./datasets/tfrecords/test/wav_test_00006-of-00010.tfrecord\n",
      "./datasets/tfrecords/test/wav_test_00003-of-00010.tfrecord\n",
      "./datasets/tfrecords/test/wav_test_00005-of-00010.tfrecord\n",
      "./datasets/tfrecords/test/wav_test_00008-of-00010.tfrecord\n",
      "./datasets/tfrecords/test/wav_test_00000-of-00010.tfrecord\n",
      "./datasets/tfrecords/test/wav_test_00001-of-00010.tfrecord\n"
     ]
    }
   ],
   "source": [
    "data_path = './datasets/tfrecords/'\n",
    "train_data_filenames = [os.path.join(data_path, 'train', name)\n",
    "                        for name in os.listdir(os.path.join(data_path, 'train')) if 'tfrecord' in name]\n",
    "for name in train_data_filenames:\n",
    "  print(name)\n",
    "print('--------')\n",
    "  \n",
    "test_data_filenames = [os.path.join(data_path, 'test', name)\n",
    "                       for name in os.listdir(os.path.join(data_path, 'test')) if 'tfrecord' in name]\n",
    "for name in test_data_filenames:\n",
    "  print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms a scalar string `example_proto` into a pair of a scalar string and\n",
    "# a scalar integer, representing an spectragram and corresponding informations, respectively.\n",
    "def _parse_function(example_proto):\n",
    "  features = {'wav_concat': tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "              'time_step': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "              'track_number': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "              'split_number': tf.io.FixedLenFeature([], tf.int64, default_value=0),}\n",
    "  \n",
    "  parsed_features = tf.io.parse_single_example(example_proto, features)\n",
    "\n",
    "  wav_concat = tf.io.decode_raw(parsed_features[\"wav_concat\"], out_type=tf.float32)\n",
    "  time_step = tf.cast(parsed_features[\"time_step\"], dtype=tf.int32)\n",
    "  #track_number = tf.cast(parsed_features[\"track_number\"], dtype=tf.int32)\n",
    "  #split_number = tf.cast(parsed_features[\"split_number\"], dtype=tf.int32)\n",
    "  \n",
    "  num_sources = 6 # for [mixtures, vocals, drums, basses, others, accompaniments]\n",
    "  num_channels = 2 # for left, right\n",
    "  wav_concat = tf.reshape(wav_concat, shape=[num_sources, num_channels, time_step])\n",
    "\n",
    "  return wav_concat, time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _augmentation_function(wav_concat, time_step):\n",
    "  \"\"\"Random cropping for data augmentation\n",
    "  We set that the input (spectrogram) shape is (num_frames, frequency_bin) = (128, 1024).\n",
    "  Number of samples to needed: 128 x 512 (hop_size) = 131072\n",
    "  Time of wave to needed: 131072 / 44100 = 2.97 sec\n",
    "  \"\"\"\n",
    "  target_time_step = 131072\n",
    "  available_time_step = time_step - target_time_step\n",
    "  \n",
    "  crop_index = tf.random.uniform(shape=[]) * tf.cast(available_time_step, dtype=tf.float32)\n",
    "  crop_index = tf.cast(crop_index, dtype=tf.int32)\n",
    "  wav_concat_crop = wav_concat[:, :, crop_index:crop_index+target_time_step]\n",
    "  \n",
    "  return wav_concat_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stft(wav, sources_to_wanted):\n",
    "  \"\"\"Short time Fourier Transform\n",
    "  \n",
    "  Args:\n",
    "    wav: Tens\n",
    "    sources_to_wanted: list of name of sources to wanted\n",
    "        - full source names: ['vocals', 'drums', 'basses', 'others', 'accompaniments']\n",
    "        \n",
    "  Returns:\n",
    "    stft: The results of stft of input wav\n",
    "  \"\"\"\n",
    "  num = collections.OrderedDict()\n",
    "  num['vocals'] = 1\n",
    "  num['drums'] = 2\n",
    "  num['basses'] = 3\n",
    "  num['others'] = 4\n",
    "  num['accompaniments'] = 5\n",
    "  \n",
    "  wav_to_wanted = collections.OrderedDict()\n",
    "  wav_to_wanted['mixtures'] = wav[0]\n",
    "  for key, value in num.items():\n",
    "    if key in sources_to_wanted:\n",
    "      wav_to_wanted[key] = wav[value]  \n",
    "      \n",
    "  stfts_db = []\n",
    "  for key, wav in wav_to_wanted.items():\n",
    "    stft = tf.contrib.signal.stft(wav,\n",
    "                          frame_length=2048,\n",
    "                          frame_step=512,\n",
    "                          fft_length=2048,\n",
    "                          pad_end=True)[..., :1024]\n",
    "    \n",
    "    # [num_channels, num_frames, frequency_bin]\n",
    "    # -> [num_frames, frequency_bin, num_channels]\n",
    "    stft = tf.transpose(stft, perm=[1, 2, 0])\n",
    "\n",
    "    def _amp_to_db(x):\n",
    "      log_offset = 1e-6\n",
    "      return 20 * tf.math.log(x + log_offset) / tf.math.log(10.0) # natural logarithm -> logarithm based on 10\n",
    "    # for our suggestion\n",
    "    min_level_db = -110\n",
    "    ref_level_db = 40\n",
    "    stft_db = _amp_to_db(tf.math.abs(stft)) - ref_level_db\n",
    "    stfts_db.append(stft_db)\n",
    "\n",
    "  return stfts_db[0], tf.concat(stfts_db[1:], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stft_for_test(wav, sources_to_wanted):\n",
    "  \"\"\"Short time Fourier Transform\n",
    "  \n",
    "  Args:\n",
    "    wav: Tens\n",
    "    sources_to_wanted: list of name of sources to wanted\n",
    "        - full source names: ['vocals', 'drums', 'basses', 'others', 'accompaniments']\n",
    "        \n",
    "  Returns:\n",
    "    mixture_stft: The real part of stft of input wav\n",
    "    targets_stft: The real part of stft of target (each source) wav\n",
    "    targets_stft_imag: The imaginary part of stft of target (each source) wav\n",
    "  \"\"\"\n",
    "  num = collections.OrderedDict()\n",
    "  num['vocals'] = 1\n",
    "  num['drums'] = 2\n",
    "  num['basses'] = 3\n",
    "  num['others'] = 4\n",
    "  num['accompaniments'] = 5\n",
    "  \n",
    "  wav_to_wanted = collections.OrderedDict()\n",
    "  wav_to_wanted['mixtures'] = wav[0]\n",
    "  for key, value in num.items():\n",
    "    if key in sources_to_wanted:\n",
    "      wav_to_wanted[key] = wav[value]  \n",
    "      \n",
    "  stfts_db = []\n",
    "  stfts_angle = []\n",
    "  for key, wav in wav_to_wanted.items():\n",
    "    stft = tf.contrib.signal.stft(wav,\n",
    "                          frame_length=2048,\n",
    "                          frame_step=512,\n",
    "                          fft_length=2048,\n",
    "                          pad_end=True)[..., :1024]\n",
    "    \n",
    "    # [num_channels, num_frames, frequency_bin]\n",
    "    # -> [num_frames, frequency_bin, num_channels]\n",
    "    stft = tf.transpose(stft, perm=[1, 2, 0])\n",
    "    \n",
    "    # separate real part and imaginary part\n",
    "    stft_angle = tf.math.angle(stft)\n",
    "    stfts_angle.append(stft_angle)\n",
    "    \n",
    "    def _amp_to_db(x):\n",
    "      log_offset = 1e-6\n",
    "      return 20 * tf.math.log(x + log_offset) / tf.math.log(10.0) # natural logarithm -> logarithm based on 10\n",
    "    # for our suggestion\n",
    "    min_level_db = -110\n",
    "    ref_level_db = 40\n",
    "    stft_db = _amp_to_db(tf.math.abs(stft)) - ref_level_db\n",
    "    stfts_db.append(stft_db)\n",
    "\n",
    "  return stfts_db[0], tf.concat(stfts_db[1:], axis=-1), tf.concat(stfts_angle[1:], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_stft(test_predictions, test_targets_angle):\n",
    "  target_shape = test_predictions.shape\n",
    "  zero_padding = tf.zeros(shape=[target_shape[0], target_shape[1], 1, target_shape[3]])\n",
    "  pred = tf.concat([test_predictions, zero_padding], axis=2)\n",
    "  angle = tf.concat([test_targets_angle, zero_padding], axis=2)\n",
    "  \n",
    "  def _db_to_amp(x):\n",
    "    return tf.math.pow(10.0, x * 0.05)\n",
    "\n",
    "  ref_level_db = 40\n",
    "  magnitude = _db_to_amp(pred + ref_level_db)\n",
    "  \n",
    "  magnitude_complex = tf.dtypes.complex(magnitude, 0.)\n",
    "  angle_complex = tf.dtypes.complex(0., angle)\n",
    "  \n",
    "  stfts = magnitude_complex * angle_complex\n",
    "  \n",
    "  # [batch_size, num_frames, frequency_bin, num_channels]\n",
    "  # -> [batch_size, num_channels, num_frames, frequency_bin]\n",
    "  stfts = tf.transpose(stfts, perm=[0, 3, 1, 2])\n",
    "  \n",
    "  inv_stfts = tf.contrib.signal.inverse_stft(stfts=stfts,\n",
    "                                     frame_length=2048,\n",
    "                                     frame_step=512)#,\n",
    "#                                     fft_length=2048)\n",
    "\n",
    "  #inv_stfts = librosa.core.istft(stft[0,0, :,:].numpy().T, hop_length=512, win_length=2048)\n",
    "  \n",
    "  return inv_stfts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(train_data_filenames)\n",
    "train_dataset = train_dataset.map(_parse_function)\n",
    "train_dataset = train_dataset.map(_augmentation_function)\n",
    "extracted_sources_list = ['vocals', 'accompaniments']\n",
    "train_dataset = train_dataset.map(lambda x: _stft(x, extracted_sources_list))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1024, 2)\n",
      "(256, 1024, 2)\n",
      "(8, 256, 1024, 4)\n",
      "duration: 0.005666971206665039 sec\n"
     ]
    }
   ],
   "source": [
    "# To test for batch\n",
    "for mixtures, targets in train_dataset.take(1):\n",
    "  start_time = time.time()\n",
    "  print(mixtures[0].shape)\n",
    "  print(mixtures[1].shape)\n",
    "  print(targets.shape)\n",
    "  print(\"duration: {} sec\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.TFRecordDataset(test_data_filenames)\n",
    "test_dataset = test_dataset.map(_parse_function)\n",
    "test_dataset = test_dataset.map(_augmentation_function)\n",
    "extracted_sources_list = ['vocals', 'accompaniments']\n",
    "test_dataset = test_dataset.map(lambda x: _stft_for_test(x, extracted_sources_list))\n",
    "test_dataset = test_dataset.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256, 1024, 2)\n",
      "(1, 256, 1024, 4)\n",
      "(1, 256, 1024, 4)\n",
      "duration: 0.0005030632019042969 sec\n"
     ]
    }
   ],
   "source": [
    "# To test for batch\n",
    "for mixtures, targets, targets_imaginary in test_dataset.take(1):\n",
    "  start_time = time.time()\n",
    "  print(mixtures.shape)\n",
    "  print(targets.shape)\n",
    "  print(targets_imaginary.shape)\n",
    "  print(\"duration: {} sec\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(tf.keras.Model):\n",
    "    \n",
    "  def __init__(self, filters, size, apply_batchnorm=True):\n",
    "    super(Downsample, self).__init__()\n",
    "    self.apply_batchnorm = apply_batchnorm\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    self.conv1 = tf.keras.layers.Conv2D(filters, \n",
    "                                        (size, size), \n",
    "                                        strides=2, \n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=False)\n",
    "    if self.apply_batchnorm:\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "  \n",
    "  def call(self, x, training):\n",
    "    x = self.conv1(x)\n",
    "    if self.apply_batchnorm:\n",
    "        x = self.batchnorm(x, training=training)\n",
    "    x = tf.nn.leaky_relu(x) # a = 0.2\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(tf.keras.Model):\n",
    "    \n",
    "  def __init__(self, filters, size, apply_dropout=False):\n",
    "    super(Upsample, self).__init__()\n",
    "    self.apply_dropout = apply_dropout\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    self.up_conv = tf.keras.layers.Conv2DTranspose(filters, \n",
    "                                                   (size, size), \n",
    "                                                   strides=2, \n",
    "                                                   padding='same',\n",
    "                                                   kernel_initializer=initializer,\n",
    "                                                   use_bias=False)\n",
    "    self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "    if self.apply_dropout:\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "  def call(self, x1, x2, training):\n",
    "    x = self.up_conv(x1)\n",
    "    x = self.batchnorm(x, training=training)\n",
    "    if self.apply_dropout:\n",
    "        x = self.dropout(x, training=training)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.concat([x, x2], axis=-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramChannelsUNet(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(SpectrogramChannelsUNet, self).__init__()\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    \n",
    "    self.down1 = Downsample(16, 5)\n",
    "    self.down2 = Downsample(32, 5)\n",
    "    self.down3 = Downsample(64, 5)\n",
    "    self.down4 = Downsample(128, 5)\n",
    "    self.down5 = Downsample(256, 5)\n",
    "    \n",
    "    self.down6 = Downsample(512, 5)\n",
    "    \n",
    "    self.up1 = Upsample(256, 5, apply_dropout=True)\n",
    "    self.up2 = Upsample(128, 5, apply_dropout=True)\n",
    "    self.up3 = Upsample(64, 5, apply_dropout=True)\n",
    "    self.up4 = Upsample(32, 5)\n",
    "    self.up5 = Upsample(16, 5)\n",
    "    \n",
    "    self.last = tf.keras.layers.Conv2DTranspose(4, #OUTPUT_CHANNELS\n",
    "                                                (5, 5), \n",
    "                                                strides=2, \n",
    "                                                padding='same',\n",
    "                                                kernel_initializer=initializer)\n",
    "  \n",
    "  @tf.contrib.eager.defun\n",
    "  def call(self, x, training):\n",
    "    # x shape == (bs, 512, 128, 1)    \n",
    "    x1 = self.down1(x, training=training) # (bs, 256, 64, 16)\n",
    "    x2 = self.down2(x1, training=training) # (bs, 128, 32, 32)\n",
    "    x3 = self.down3(x2, training=training) # (bs, 64, 16, 64)\n",
    "    x4 = self.down4(x3, training=training) # (bs, 32, 8, 128)\n",
    "    x5 = self.down5(x4, training=training) # (bs, 16, 4, 256)\n",
    "    \n",
    "    x6 = self.down6(x5, training=training) # (bs, 8, 2, 512) - Center    \n",
    "\n",
    "    x7 = self.up1(x6, x5, training=training) # (bs, 16, 4, 256)\n",
    "    x8 = self.up2(x7, x4, training=training) # (bs, 32, 8, 128)\n",
    "    x9 = self.up3(x8, x3, training=training) # (bs, 64, 16, 64)\n",
    "    x10 = self.up4(x9, x2, training=training) # (bs, 128, 32, 32)\n",
    "    x11 = self.up5(x10, x1, training=training) # (bs, 256, 64, 16)\n",
    "    \n",
    "    x12 = self.last(x11) # (bs, 512, 128, 1)\n",
    "    x13 = tf.nn.tanh(x12) # (bs, 512, 128, 1)\n",
    "    \n",
    "    #mask = np.multiply(x, x13)\n",
    "    \n",
    "    return x13 #training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpectrogramChannelsUNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fetch inputs to model in order to set to input shape\n",
    "for mixtures, targets in train_dataset.take(1):\n",
    "  predictions = model(mixtures, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "downsample_12 (Downsample)   multiple                  864       \n",
      "_________________________________________________________________\n",
      "downsample_13 (Downsample)   multiple                  12928     \n",
      "_________________________________________________________________\n",
      "downsample_14 (Downsample)   multiple                  51456     \n",
      "_________________________________________________________________\n",
      "downsample_15 (Downsample)   multiple                  205312    \n",
      "_________________________________________________________________\n",
      "downsample_16 (Downsample)   multiple                  820224    \n",
      "_________________________________________________________________\n",
      "downsample_17 (Downsample)   multiple                  3278848   \n",
      "_________________________________________________________________\n",
      "upsample_10 (Upsample)       multiple                  3277824   \n",
      "_________________________________________________________________\n",
      "upsample_11 (Upsample)       multiple                  1638912   \n",
      "_________________________________________________________________\n",
      "upsample_12 (Upsample)       multiple                  409856    \n",
      "_________________________________________________________________\n",
      "upsample_13 (Upsample)       multiple                  102528    \n",
      "_________________________________________________________________\n",
      "upsample_14 (Upsample)       multiple                  25664     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DT multiple                  3204      \n",
      "=================================================================\n",
      "Total params: 9,827,620\n",
      "Trainable params: 9,824,612\n",
      "Non-trainable params: 3,008\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singing_voice_loss(y_true, y_pred, alpha=1.0):\n",
    "  # Use the mean_absolute_error\n",
    "  #mae = tf.keras.losses.MeanAbsoluteError()\n",
    "  #loss_vocal = mae(y_true[..., :2], y_pred[..., :2])\n",
    "  #loss_accompaniments = mae(y_true[..., 2:], y_pred[..., 2:])\n",
    "  \n",
    "  loss_vocal = tf.losses.absolute_difference(y_true[..., :2], y_pred[..., :2])\n",
    "  loss_accompaniments = tf.losses.absolute_difference(y_true[..., 2:], y_pred[..., 2:])\n",
    "  \n",
    "  return alpha * loss_vocal + (1. - alpha) * loss_accompaniments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = train_dir\n",
    "#if not tf.io.gfile.exists(checkpoint_dir):\n",
    "#  tf.io.gfile.makedirs(checkpoint_dir)\n",
    "if not tf.gfile.Exists(checkpoint_dir):\n",
    "  tf.gfile.MakeDirs(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 0.19 global_step: 3 loss: 104.520 (105.21 examples/sec; 0.076 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "print('Start Training.')\n",
    "global_step = 1\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "#for epoch in range(1):\n",
    "  \n",
    "  for mixtures, targets in train_dataset.take(1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "      predictions = model(mixtures, training=True)\n",
    "      loss = singing_voice_loss(targets, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.variables))\n",
    "        \n",
    "    epochs = global_step * batch_size / float(N)\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    if global_step % print_steps == 0:\n",
    "      display.clear_output(wait=True)\n",
    "      examples_per_sec = batch_size / float(duration)\n",
    "      print(\"Epochs: {:.2f} global_step: {} loss: {:.3f} ({:.2f} examples/sec; {:.3f} sec/batch)\".format(\n",
    "                epochs, global_step, loss, examples_per_sec, duration))\n",
    "      for test_mixtures, test_targets, test_targets_angle in test_dataset.take(1):\n",
    "        test_predictions = model(test_mixtures, training=False)\n",
    "        test_wav = inverse_stft(test_predictions, test_targets_angle)\n",
    "    global_step = global_step + 1\n",
    "\n",
    "  # saving (checkpoint) the model every save_epochs\n",
    "  if (epoch + 1) % save_model_epochs == 0:\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_mixtures, test_targets, test_targets_angle in test_dataset.take(1):\n",
    "  test_predictions = model(test_mixtures, training=False)\n",
    "  test_wav = inverse_stft(test_predictions, test_targets_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(test_wav[0, :2, :], rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 1.12 on Python 3.6 (CUDA 9.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
